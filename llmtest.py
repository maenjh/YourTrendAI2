#######################################
# streamlit_app.py
#######################################
import streamlit as st
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from typing import List, Dict

#######################################
# 1) ëª¨ë¸ ë¡œë”© ë° í…ìŠ¤íŠ¸ ìƒì„± í•¨ìˆ˜
#######################################
MODEL_PATH = "/workspace/models/Llama-2-7b-hf"  # ì‹¤ì œ ëª¨ë¸ ê²½ë¡œë¡œ ìˆ˜ì •í•˜ì„¸ìš”

@st.cache_resource
def load_model():
    """
    ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë”©í•˜ì—¬ ë¦¬í„´í•©ë‹ˆë‹¤.
    streamlitì—ì„œ @st.cache_resource ë°ì½”ë ˆì´í„°ë¡œ 
    ë§¤ë²ˆ ì¬ë¡œë”©ë˜ì§€ ì•Šë„ë¡ ìºì‹±í•©ë‹ˆë‹¤.
    """
    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_PATH,
        torch_dtype=torch.float16,   # GPU ì‚¬ìš© ì‹œ float16 ê¶Œì¥
        device_map="auto"           # GPU ì—¬ëŸ¬ ì¥ì´ ìˆì„ ê²½ìš° ìë™ ë¶„ì‚°
    )
    return tokenizer, model

def hf_generate_text(
    tokenizer,
    model,
    prompt: str,
    temperature: float = 0.7,
    max_new_tokens: int = 500
) -> str:
    """
    Llama 2 HF ëª¨ë¸ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” í—¬í¼ í•¨ìˆ˜.
    (pipeline ëŒ€ì‹  model.generate()ë¥¼ ì§ì ‘ ì‚¬ìš©í•œ ì˜ˆì‹œ)
    """
    input_ids = tokenizer.encode(prompt, return_tensors="pt").to(model.device)
    output_ids = model.generate(
        input_ids,
        do_sample=True,
        temperature=temperature,
        max_new_tokens=max_new_tokens,
        top_p=0.9
    )
    generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)

    # í”„ë¡¬í”„íŠ¸ ì œê±°(ì˜µì…˜)
    prompt_len = len(tokenizer.decode(input_ids[0], skip_special_tokens=True))
    return generated_text[prompt_len:].strip()


#######################################
# 2) ì—ì´ì „íŠ¸/íšŒì˜ ë¡œì§
#######################################
class AIAgent:
    """
    ê° ì „ë¬¸ê°€(ì‹œì¥ë¶„ì„ê°€, í”„ë¡œë•íŠ¸ ë§¤ë‹ˆì € ë“±)ë¥¼ ë‚˜íƒ€ë‚´ëŠ” í´ë˜ìŠ¤.
    HF ëª¨ë¸(generate_text)ì„ í˜¸ì¶œí•´ ë‹µë³€ì„ ìƒì„±.
    """
    def __init__(self, name: str, role: str, temperature: float, personality: str):
        self.name = name
        self.role = role
        self.temperature = temperature
        self.personality = personality
        self.conversation_history: List[Dict] = []

    def generate_response(
        self,
        tokenizer,
        model,
        topic: str,
        other_response: str = "",
        context: str = "",
        round_num: int = 1
    ) -> str:
        """
        round_num == 1: (ì´ˆê¸° ì˜ê²¬)
        round_num > 1 : (ì§§ì€ í”¼ë“œë°± ë° ì œì•ˆ)
        """
        if round_num == 1:
            # 1ë¼ìš´ë“œ: ìƒì„¸ ì˜ê²¬
            prompt = f"""
ë‹¹ì‹ ì€ {self.name}ì´ë©°, {self.role}ì…ë‹ˆë‹¤.
ì„±ê²©ê³¼ ë§íˆ¬: {self.personality}

í† ë¡  ì£¼ì œ: {topic}

ë¶„ì„í•  ì½˜í…ì¸ :
{context}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì˜ê²¬ì„ ì œì‹œí•´ì£¼ì„¸ìš”:
1. í˜„ì¬ ìƒí™© ë¶„ì„
2. ê¸°íšŒ ìš”ì†Œ ë°œê²¬
3. í•´ê²° ë°©ì•ˆ ì œì‹œ
4. êµ¬ì²´ì  ì‹¤í–‰ ê³„íš
5. ì˜ˆìƒë˜ëŠ” ë„ì „ ê³¼ì œ
"""
        else:
            # 2ë¼ìš´ë“œ ì´ìƒ: ì§§ì€ í”¼ë“œë°±
            prompt = f"""
ë‹¹ì‹ ì€ {self.name}ì´ë©°, {self.role}ì…ë‹ˆë‹¤.
ì„±ê²©ê³¼ ë§íˆ¬: {self.personality}

ì´ì „ ëŒ€í™”:
{other_response}

ìœ„ ë‚´ìš©ì— ëŒ€í•œ ì§§ì€ í”¼ë“œë°±ê³¼ ì œì•ˆì„ 200ì ì´ë‚´ë¡œ ì œì‹œí•´ì£¼ì„¸ìš”.
"""

        try:
            generated_response = hf_generate_text(
                tokenizer=tokenizer,
                model=model,
                prompt=prompt,
                temperature=self.temperature,
                max_new_tokens=600
            ).strip()
            self.conversation_history.append({"role": "assistant", "content": generated_response})
            return generated_response

        except Exception as e:
            return f"ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"


def create_context(transcripts: List[str], video_urls: List[str]) -> str:
    """
    ì—¬ëŸ¬ ì˜ìƒì—ì„œ ì¶”ì¶œí•œ ìŠ¤í¬ë¦½íŠ¸/URLì„ í•˜ë‚˜ì˜ ë¬¸ìì—´ ì»¨í…ìŠ¤íŠ¸ë¡œ í•©ì¹˜ëŠ” ì˜ˆì‹œ.
    """
    return "".join([
        f"\n[ì˜ìƒ {i+1}] URL: {url}\nì˜ìƒ ë‚´ìš© ìš”ì•½:\n{transcript}\n{'-'*50}"
        for i, (transcript, url) in enumerate(zip(transcripts, video_urls))
    ])

def display_message(agent_name: str, message: str):
    """
    Streamlit í™˜ê²½ì—ì„œ ì—ì´ì „íŠ¸ë³„ ë©”ì‹œì§€ë¥¼ ì˜ˆì˜ê²Œ ë³´ì—¬ì£¼ê¸° ìœ„í•œ ìœ í‹¸.
    """
    style = {
        "ì‹œì¥ë¶„ì„ê°€": {"bg_color": "#E8F4F9", "border_color": "#2196F3", "icon": "ğŸ“Š"},
        "í”„ë¡œë•íŠ¸ ë§¤ë‹ˆì €": {"bg_color": "#F3E5F5", "border_color": "#9C27B0", "icon": "ğŸ’¡"},
        "í…Œí¬ë¦¬ë“œ": {"bg_color": "#E8F5E9", "border_color": "#4CAF50", "icon": "âš™ï¸"},
        "ì‚¬ì—…ì „ëµê°€": {"bg_color": "#FFF3E0", "border_color": "#FF9800", "icon": "ğŸ“ˆ"}
    }
    agent_style = style.get(agent_name, {
        "bg_color": "#F5F5F5",
        "border_color": "#9E9E9E",
        "icon": "ğŸ’­"
    })

    st.markdown(f"""
    <div style="
        background-color: {agent_style['bg_color']};
        padding: 15px;
        border-radius: 10px;
        margin: 10px 0;
        border-left: 4px solid {agent_style['border_color']};
    ">
        <strong>{agent_style['icon']} {agent_name}</strong><br>{message}
    </div>
    """, unsafe_allow_html=True)


def generate_final_summary(
    tokenizer,
    model,
    conversation: List[dict],
    user_prompt: str
) -> str:
    """
    ëª¨ë“  ë¼ìš´ë“œì—ì„œ ë‚˜ì™”ë˜ ì „ë¬¸(ì—ì´ì „íŠ¸) ì˜ê²¬ì„ ìš”ì•½í•˜ì—¬ ìµœì¢… ì œì•ˆì„œ ìƒì„±.
    """
    # íšŒì˜ ë‚´ìš©ì„ ê°„ë‹¨íˆ í•©ì¹œ í…ìŠ¤íŠ¸
    conversation_summary = "\n\n".join([
        f"ë¼ìš´ë“œ {msg['round']} - {msg['agent']}: {msg['response']}"
        for msg in conversation
    ])

    prompt = f"""
ì£¼ì œ: {user_prompt}

ì „ë¬¸ê°€ë“¤ì˜ ë…¼ì˜ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ í•µì‹¬ì ì¸ ì œì•ˆ ì‚¬í•­ë§Œ ê°„ë‹¨íˆ ì •ë¦¬í•´ì£¼ì„¸ìš”:

1. í”„ë¡œì íŠ¸ ê°œìš”
2. í•µì‹¬ ê¸°ëŠ¥
3. ê¸°ìˆ  êµ¬í˜„ ë°©ì•ˆ
4. ë¹„ì¦ˆë‹ˆìŠ¤ ì „ëµ
5. ì‹¤í–‰ ê³„íš
6. ì£¼ìš” ê³ ë ¤ì‚¬í•­

ì „ë¬¸ê°€ ë…¼ì˜ ë‚´ìš©:
{conversation_summary}
"""
    try:
        summary = hf_generate_text(tokenizer, model, prompt, temperature=0.7, max_new_tokens=1000)
        return summary.strip()
    except Exception as e:
        return f"ìµœì¢… ìš”ì•½ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"


def generate_discussion(
    tokenizer,
    model,
    transcripts: List[str],
    video_urls: List[str],
    user_prompt: str,
    num_rounds: int = 3
) -> tuple:
    """
    - 4ëª…(ì‹œì¥ë¶„ì„ê°€Â·í”„ë¡œë•íŠ¸ ë§¤ë‹ˆì €Â·í…Œí¬ë¦¬ë“œÂ·ì‚¬ì—…ì „ëµê°€)ì´ ë‹¤ì¤‘ ë¼ìš´ë“œ íšŒì˜ë¥¼ ì§„í–‰.
    - ê° ë¼ìš´ë“œì—ì„œ ë²ˆê°ˆì•„ ê°€ë©° ì˜ê²¬(ë‹µë³€)ì„ ìƒì„±.
    - ëª¨ë“  ë¼ìš´ë“œê°€ ëë‚˜ë©´ ìµœì¢… ìš”ì•½ì„ ìƒì„±.
    """
    # 1) ì—ì´ì „íŠ¸(ì „ë¬¸ê°€) ìƒì„±
    analyst = AIAgent(
        name="ì‹œì¥ë¶„ì„ê°€",
        role="ì‹œì¥ íŠ¸ë Œë“œì™€ ì‚¬ìš©ì ë‹ˆì¦ˆ ë¶„ì„ ì „ë¬¸ê°€",
        temperature=0.7,
        personality="ë°ì´í„° ê¸°ë°˜ì˜ ê°ê´€ì ì¸ ë¶„ì„ì„ ì œê³µí•˜ë©°, ì‹œì¥ì˜ ê¸°íšŒì™€ ìœ„í—˜ ìš”ì†Œë¥¼ íŒŒì•…í•©ë‹ˆë‹¤."
    )
    product_manager = AIAgent(
        name="í”„ë¡œë•íŠ¸ ë§¤ë‹ˆì €",
        role="ì œí’ˆ ê¸°íš ë° ì „ëµ ìˆ˜ë¦½ ì „ë¬¸ê°€",
        temperature=0.8,
        personality="ì‚¬ìš©ì ì¤‘ì‹¬ì  ì‚¬ê³ ì™€ ë¹„ì¦ˆë‹ˆìŠ¤ ê°€ì¹˜ë¥¼ ê· í˜•ìˆê²Œ ê³ ë ¤í•©ë‹ˆë‹¤."
    )
    tech_lead = AIAgent(
        name="í…Œí¬ë¦¬ë“œ",
        role="ê¸°ìˆ  êµ¬í˜„ ë° ì•„í‚¤í…ì²˜ ì„¤ê³„ ì „ë¬¸ê°€",
        temperature=0.7,
        personality="ìµœì‹  ê¸°ìˆ  íŠ¸ë Œë“œë¥¼ ì´í•´í•˜ê³  ì‹¤ì œ êµ¬í˜„ ê°€ëŠ¥ì„±ì„ í‰ê°€í•©ë‹ˆë‹¤."
    )
    business_strategist = AIAgent(
        name="ì‚¬ì—…ì „ëµê°€",
        role="ë¹„ì¦ˆë‹ˆìŠ¤ ëª¨ë¸ ë° ìˆ˜ìµí™” ì „ëµ ì „ë¬¸ê°€",
        temperature=0.8,
        personality="ì‹œì¥ì„±ê³¼ ìˆ˜ìµì„±ì„ ê³ ë ¤í•œ ì‚¬ì—… ì „ëµì„ ìˆ˜ë¦½í•©ë‹ˆë‹¤."
    )

    # 2) ì˜ìƒ ìŠ¤í¬ë¦½íŠ¸ + URL ì»¨í…ìŠ¤íŠ¸ ë§Œë“¤ê¸°
    context = create_context(transcripts, video_urls)
    conversation = []
    agents = [analyst, product_manager, tech_lead, business_strategist]

    # 3) ë¼ìš´ë“œ ì§„í–‰
    for round_num in range(num_rounds):
        st.markdown(f"### ğŸ”„ ë¼ìš´ë“œ {round_num + 1}")
        # 4ëª…ì˜ ì „ë¬¸ê°€ê°€ ì°¨ë¡€ë¡œ ì˜ê²¬
        for agent in agents:
            with st.spinner(f'{agent.name}ì˜ ì˜ê²¬ì„ ë¶„ì„ ì¤‘...'):
                # ë°”ë¡œ ì´ì „ ë¼ìš´ë“œê¹Œì§€ì˜ íƒ€ ì „ë¬¸ê°€ ë°œì–¸ë“¤(ìµœëŒ€ 4ê°œ)ë§Œ ì°¸ì¡° ì˜ˆì‹œ
                other_responses = "\n\n".join([
                    f"{msg['agent']}: {msg['response']}"
                    for msg in conversation[-4:] if msg['agent'] != agent.name
                ])

                response = agent.generate_response(
                    tokenizer=tokenizer,
                    model=model,
                    topic=user_prompt,
                    other_response=other_responses,
                    context=context,
                    round_num=round_num + 1
                )
                conversation.append({
                    "agent": agent.name,
                    "response": response,
                    "round": round_num + 1
                })
                display_message(agent.name, response)

        st.markdown(f"""
        <div style="padding: 10px; margin: 20px 0; text-align: center; background-color: #f0f2f6; border-radius: 10px;">
            âœ¨ ë¼ìš´ë“œ {round_num + 1} ì™„ë£Œ
        </div>
        """, unsafe_allow_html=True)

    # 4) ìµœì¢… ìš”ì•½
    final_summary = generate_final_summary(tokenizer, model, conversation, user_prompt)
    return final_summary, conversation


#######################################
# 3) Streamlit ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
#######################################
def main():
    st.title("Llama 2 ê¸°ë°˜ ë‹¤ì¤‘ ì „ë¬¸ê°€ í† ë¡  ë°ëª¨")
    st.write("4ëª…ì˜ ì „ë¬¸ê°€(ì‹œì¥ë¶„ì„ê°€, í”„ë¡œë•íŠ¸ ë§¤ë‹ˆì €, í…Œí¬ë¦¬ë“œ, ì‚¬ì—…ì „ëµê°€)ê°€ ë¼ìš´ë“œë³„ë¡œ ì˜ê²¬ì„ ì œì‹œí•˜ê³ , ë§ˆì§€ë§‰ì— ì¢…í•© ìš”ì•½ì„ ìƒì„±í•©ë‹ˆë‹¤.")

    # (1) ì‚¬ìš©ì ì…ë ¥: ì£¼ì œ
    user_prompt = st.text_input("í† ë¡ í•˜ê³  ì‹¶ì€ ì£¼ì œë¥¼ ì…ë ¥í•˜ì„¸ìš”", value="AI ìŠ¤íƒ€íŠ¸ì—… ì•„ì´ë””ì–´")

    # (2) ì˜ˆì‹œ ì˜ìƒ ìŠ¤í¬ë¦½íŠ¸ ì…ë ¥ (ì‹¤ì œë¡œëŠ” íŒŒì¼ ì—…ë¡œë“œë‚˜ API ê²°ê³¼ì¼ ìˆ˜ ìˆìŒ)
    st.write("ì˜ìƒ ìŠ¤í¬ë¦½íŠ¸ ë° URLì„ ì˜ˆì‹œë¡œ ì…ë ¥í•´ ì£¼ì„¸ìš”.")
    video1_url = st.text_input("ì˜ìƒ 1 URL", "https://youtube.com/example1")
    video1_script = st.text_area("ì˜ìƒ 1 ìŠ¤í¬ë¦½íŠ¸ ìš”ì•½", "AI ì‹œì¥ í˜„í™©ì— ëŒ€í•œ ë‚´ìš©...")

    video2_url = st.text_input("ì˜ìƒ 2 URL", "https://youtube.com/example2")
    video2_script = st.text_area("ì˜ìƒ 2 ìŠ¤í¬ë¦½íŠ¸ ìš”ì•½", "ë”¥ëŸ¬ë‹ ëª¨ë¸ ì„œë¹„ìŠ¤í™” ì „ëµ...")

    # í•„ìš”í•œ ê²½ìš° ì¶”ê°€ ì˜ìƒë„ ë°›ì„ ìˆ˜ ìˆìŒ
    transcripts = [video1_script, video2_script]
    video_urls = [video1_url, video2_url]

    # (3) ë¼ìš´ë“œ ìˆ˜ ì„¤ì •
    num_rounds = st.slider("ë¼ìš´ë“œ ìˆ˜", 1, 5, 3)

    # (4) ë²„íŠ¼ í´ë¦­ ì‹œ í† ë¡  ì‹¤í–‰
    if st.button("í† ë¡  ì‹œì‘"):
        # ëª¨ë¸ ë¡œë“œ
        with st.spinner("ëª¨ë¸ ë¡œë”© ì¤‘... (ì²˜ìŒ í•œ ë²ˆë§Œ ì˜¤ë˜ ê±¸ë¦½ë‹ˆë‹¤)"):
            tokenizer, model = load_model()

        final_summary, conversation = generate_discussion(
            tokenizer,
            model,
            transcripts=transcripts,
            video_urls=video_urls,
            user_prompt=user_prompt,
            num_rounds=num_rounds
        )

        # ìµœì¢… ìš”ì•½ ê²°ê³¼ í‘œì‹œ
        st.subheader("âœ… ìµœì¢… ì œì•ˆ ìš”ì•½")
        st.write(final_summary)


if __name__ == "__main__":
    main()
